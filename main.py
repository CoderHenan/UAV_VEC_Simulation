# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import os
import shutil
import time
import torch
from config import cfg
from env_core import UAVEnv
from agent import ST_MASAC_Agent, DDPG_Agent, DQN_Agent, QLearning_Agent, AC_Agent, Random_Agent


def run_experiment(algo_name):
    print(f"==================================================")
    print(f"   ğŸš€ STARTING EXPERIMENT: {algo_name}")
    print(f"   ğŸ“‚ Exp Name: {cfg.EXP_NAME}")
    print(f"==================================================")

    # 1. åˆå§‹åŒ–ç¯å¢ƒä¸æ™ºèƒ½ä½“
    env = UAVEnv()

    agents_map = {
        "ST-C-MASAC": ST_MASAC_Agent,
        "DDPG": DDPG_Agent,
        "DQN": DQN_Agent,
        "AC": AC_Agent,
        "Q-Learning": QLearning_Agent,
        "Random": Random_Agent
    }

    agent_cls = agents_map.get(algo_name)
    if agent_cls is None:
        print(f"!! Error: Agent {algo_name} not implemented.")
        return
    agent = agent_cls()

    # 2. è·¯å¾„è®¾ç½®
    os.makedirs(cfg.RESULT_PATH, exist_ok=True)
    model_dir = os.path.join(cfg.MODEL_PATH, algo_name)
    os.makedirs(model_dir, exist_ok=True)
    csv_path = os.path.join(cfg.RESULT_PATH, f"{algo_name}_metrics.csv")

    # 3. æ–­ç‚¹ç»­è®­é€»è¾‘ (Smart Resume)
    start_ep = 0

    # [ä¿®æ”¹] ä¼˜å…ˆå°è¯•åŠ è½½å®Œæ•´çš„ Checkpoint (åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€)
    if hasattr(agent, 'load_ckpt'):
        loaded_ep = agent.load_ckpt(model_dir)
        if loaded_ep > 0:
            start_ep = loaded_ep
            print(f"âœ… Resumed training from Checkpoint: Episode {start_ep}")

    # å…¼å®¹æ—§ç‰ˆåŠ è½½é€»è¾‘
    elif hasattr(agent, 'load') and os.path.exists(csv_path):
        try:
            df_hist = pd.read_csv(csv_path)
            if not df_hist.empty and agent.load(model_dir):
                start_ep = int(df_hist.iloc[-1, 0]) + 1
                print(f"âš ï¸ Resumed using Legacy Load (Weights Only) from Episode {start_ep}")
        except:
            pass

    # åˆå§‹åŒ– CSV
    if start_ep == 0 or not os.path.exists(csv_path):
        with open(csv_path, 'w') as f:
            f.write("ep,reward,delay,energy,succ\n")

    # 4. ä¸»è®­ç»ƒå¾ªç¯
    for ep in range(start_ep, cfg.MAX_EPISODES):
        try:
            st_time = time.time()
            obs, _ = env.reset()

            # [ç‰¹æœ‰] é‡ç½® Frame Stack
            if hasattr(agent, 'reset_stack'):
                obs = agent.reset_stack(obs)

            ep_r, ep_delay, ep_energy, ep_succ = 0, 0, 0, 0
            actual_steps = 0

            for step in range(cfg.MAX_STEPS):
                # é€‰æ‹©åŠ¨ä½œ
                action, h_in, c_in, h_out, c_out = agent.select_action(obs, noise=True)

                next_obs, next_g, rewards, done, info = env.step(action)

                # å †å è§‚æµ‹
                if hasattr(agent, 'stack_obs'):
                    next_obs_processed = agent.stack_obs(next_obs)
                else:
                    next_obs_processed = next_obs

                # ç®—æ³•æ›´æ–°
                if algo_name in ["Random"]:
                    pass
                elif algo_name in ["Q-Learning", "AC"]:
                    agent.update_step(obs, action, rewards, next_obs_processed, done)
                else:
                    # Off-Policy (DDPG, DQN, ST-C-MASAC)
                    if hasattr(agent, 'memory'):
                        agent.memory.push(obs, action, rewards, next_obs_processed, done, h_in, c_in, h_out, c_out)
                        agent.update()

                obs = next_obs_processed
                ep_r += np.sum(rewards)
                ep_delay += info['delay']
                ep_energy += info['energy']
                ep_succ += info['succ']
                actual_steps += 1

                if np.all(done): break

            # [ä¿®æ”¹] Episode ç»“æŸï¼Œæ›´æ–°å­¦ä¹ ç‡ (Scheduler Step)
            if hasattr(agent, 'update_lr'):
                agent.update_lr()

            # ç»Ÿè®¡ä¸è®°å½•
            avg_delay = ep_delay / max(1, actual_steps)
            avg_energy = ep_energy / max(1, actual_steps)
            fps = actual_steps / (time.time() - st_time)

            log_str = f"{ep},{ep_r:.4f},{avg_delay:.4f},{avg_energy:.4f},{ep_succ}\n"
            with open(csv_path, 'a') as f:
                f.write(log_str)

            # [ä¿®æ”¹] æ—¥å¿—è¾“å‡ºï¼šå¢åŠ  LR å’Œ Q å€¼ç›‘æ§
            if ep % 10 == 0:
                # è·å–å½“å‰å­¦ä¹ ç‡
                curr_lr = 0.0
                if hasattr(agent, 'actor_opts') and agent.actor_opts:
                    curr_lr = agent.actor_opts[0].param_groups[0]['lr']

                msg = f"Ep {ep:<4} | R: {ep_r:>7.1f} | D: {avg_delay:>5.3f} | Succ: {ep_succ:>2} | FPS: {int(fps)}"

                # è·å– SAC å†…éƒ¨è¯Šæ–­ä¿¡æ¯
                if hasattr(agent, 'log_alpha'):  # ç®€å•åˆ¤æ–­æ˜¯å¦æ˜¯ SAC ç±»
                    # å°è¯•è¯»å–å†…éƒ¨å˜é‡ (å‡è®¾ agent å­˜äº†è¿™äº›ä¸´æ—¶å˜é‡ï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿæ²¡å…³ç³»)
                    # æ›´å¥½çš„æ–¹å¼æ˜¯ agent.update() è¿”å› infoï¼Œä½†ä¸ºäº†ä¸æ”¹åŠ¨å¤ªå¤§ï¼Œè¿™é‡Œåªæ‰“å° LR
                    msg += f" | LR: {curr_lr:.2e}"

                print(msg)

            # [ä¿®æ”¹] å®šæœŸä¿å­˜ Checkpoint
            if ep % 20 == 0:
                if hasattr(agent, 'save_ckpt'):
                    agent.save_ckpt(model_dir, ep)
                elif hasattr(agent, 'save'):
                    agent.save(model_dir)

        except KeyboardInterrupt:
            print("\nğŸ›‘ Training interrupted. Saving checkpoint...")
            if hasattr(agent, 'save_ckpt'):
                agent.save_ckpt(model_dir, ep)
            return
        except Exception as e:
            print(f"\nâŒ Error in Episode {ep}: {e}")
            import traceback
            traceback.print_exc()
            if hasattr(agent, 'save_ckpt'):
                agent.save_ckpt(model_dir, ep)
            break

    print(f"\nâœ… Experiment Finished: {algo_name}")


if __name__ == "__main__":
    # algos = ["ST-C-MASAC", "DDPG"]
    # algos = ["ST-C-MASAC"]
    # algos = ["ST-C-MASAC"]
    algos = ["DDPG"]
    # algos = ["Q-Learning"]    # å·²æµ‹è¯•
    # algos = ["Random"]        # å·²æµ‹è¯•
    for algo in algos:
        run_experiment(algo)