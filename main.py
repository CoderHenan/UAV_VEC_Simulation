# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import os
import time
import torch
from config import cfg
from env_core import UAVEnv
from agent import ST_MASAC_Agent, DDPG_Agent, DQN_Agent, QLearning_Agent, AC_Agent, Random_Agent


def run_experiment(algo_name):
    print(f"\n{'=' * 60}")
    print(f"ğŸš€ å¯åŠ¨å®éªŒ: {algo_name}")
    print(f"ğŸ“‚ å®éªŒç›®å½•: {cfg.EXP_NAME}")
    print(f"{'=' * 60}")

    # 1. åˆå§‹åŒ–ç¯å¢ƒ
    env = UAVEnv()

    agents_map = {
        "ST-C-MASAC": ST_MASAC_Agent,
        "DDPG": DDPG_Agent,
        "DQN": DQN_Agent,
        "AC": AC_Agent,
        "Q-Learning": QLearning_Agent,
        "Random": Random_Agent
    }

    if algo_name not in agents_map:
        print(f"!! é”™è¯¯: æœªå®ç°çš„ç®—æ³• {algo_name}")
        return

    agent = agents_map[algo_name]()

    # [æ£€æŸ¥ç‚¹] æ‰“å°å½“å‰ç®—æ³•æ˜¯å¦ä½¿ç”¨äº† Frame Stack
    # è¿™æ˜¯åŒºåˆ† v16 ä¸»è§’ä¸é…è§’çš„å…³é”®æ ‡å¿—
    use_stack = hasattr(agent, 'stack_obs')
    obs_dim_used = cfg.OBS_DIM if use_stack else cfg.RAW_OBS_DIM

    print(f"â„¹ï¸  ç®—æ³•é…ç½®æ£€æŸ¥:")
    print(f"   - Frame Stack: {'âœ… ENABLED (24-dim)' if use_stack else 'âŒ DISABLED (8-dim Baseline)'}")
    print(f"   - Obs Dim: {obs_dim_used}")
    print(f"   - Device: {cfg.DEVICE}")
    print(f"{'-' * 60}\n")

    # 2. è·¯å¾„ä¸æ¢å¤
    os.makedirs(cfg.RESULT_PATH, exist_ok=True)
    model_dir = os.path.join(cfg.MODEL_PATH, algo_name)
    os.makedirs(model_dir, exist_ok=True)
    csv_path = os.path.join(cfg.RESULT_PATH, f"{algo_name}_metrics.csv")

    start_ep = 0
    # [ä¿®æ­£] ä¼˜å…ˆè°ƒç”¨å®Œæ•´çŠ¶æ€åŠ è½½
    if hasattr(agent, 'load_ckpt'):
        start_ep = agent.load_ckpt(model_dir)
        if start_ep > 0: print(f"âœ… æ–­ç‚¹ç»­è®­: ä» Ep {start_ep} å¼€å§‹")
    elif hasattr(agent, 'load') and os.path.exists(csv_path):  # å…¼å®¹æ—§ç‰ˆé€»è¾‘
        try:
            df = pd.read_csv(csv_path)
            if not df.empty and agent.load(model_dir):
                start_ep = int(df.iloc[-1, 0]) + 1
        except:
            pass

    # åˆå§‹åŒ– CSV è¡¨å¤´
    if start_ep == 0 or not os.path.exists(csv_path):
        with open(csv_path, 'w') as f:
            f.write("ep,reward,delay,energy,succ\n")

    # 3. è®­ç»ƒå¾ªç¯
    for ep in range(start_ep, cfg.MAX_EPISODES):
        try:
            st_time = time.time()
            raw_obs, _ = env.reset()

            # [å…³é”®åˆ†æµ] åªæœ‰ ST-C-MASAC ä¼šåœ¨è¿™é‡Œåˆå§‹åŒ– Stack
            if use_stack:
                curr_state = agent.reset_stack(raw_obs)
            else:
                curr_state = raw_obs  # DDPG/DQN ç›´æ¥ç”¨åŸå§‹è§‚æµ‹

            ep_r, ep_delay, ep_energy, ep_succ = 0, 0, 0, 0
            steps = 0

            for step in range(cfg.MAX_STEPS):
                # åŠ¨ä½œé€‰æ‹© (Training mode: noise=True)
                # æ³¨æ„ï¼šagent.py ä¸­æ‰€æœ‰ç®—æ³•çš„ select_action æ¥å£å·²å¯¹é½ï¼Œè¿”å›5ä¸ªå€¼
                action, h_in, c_in, h_out, c_out = agent.select_action(curr_state, noise=True)

                next_raw_obs, _, rewards, done, info = env.step(action)

                # [å…³é”®åˆ†æµ] çŠ¶æ€è½¬æ¢
                if use_stack:
                    next_state = agent.stack_obs(next_raw_obs)
                else:
                    next_state = next_raw_obs

                # å­˜å‚¨ä¸æ›´æ–°
                if algo_name in ["Random"]:
                    pass
                elif hasattr(agent, 'memory'):  # Off-Policy (DDPG, DQN, MASAC)
                    agent.memory.push(curr_state, action, rewards, next_state, done, h_in, c_in, h_out, c_out)
                    agent.update()
                elif hasattr(agent, 'update_step'):  # On-Policy / Tabular (AC, QL)
                    agent.update_step(curr_state, action, rewards, next_state, done)

                curr_state = next_state
                ep_r += np.sum(rewards)
                ep_delay += info['delay']
                ep_energy += info['energy']
                ep_succ += info['succ']
                steps += 1

                if np.all(done): break

            # [ä¿®æ­£] Episode ç»“æŸï¼Œæ›´æ–°å­¦ä¹ ç‡ (ä»…æ”¯æŒ Scheduler çš„ Agent æœ‰æ­¤æ–¹æ³•)
            if hasattr(agent, 'update_lr'):
                agent.update_lr()

            # è®°å½•
            avg_delay = ep_delay / max(1, steps)
            avg_energy = ep_energy / max(1, steps)
            fps = int(steps / (time.time() - st_time))

            with open(csv_path, 'a') as f:
                f.write(f"{ep},{ep_r:.4f},{avg_delay:.4f},{avg_energy:.4f},{ep_succ}\n")

            # æ‰“å°æ—¥å¿—
            if ep % 10 == 0:
                lr_str = ""
                # è·å– LR ç”¨äºç›‘æ§
                if hasattr(agent, 'actor_opts') and len(agent.actor_opts) > 0:
                    curr_lr = agent.actor_opts[0].param_groups[0]['lr']
                    lr_str = f"| LR: {curr_lr:.2e}"
                elif hasattr(agent, 'opts') and len(agent.opts) > 0:  # DQN
                    curr_lr = agent.opts[0].param_groups[0]['lr']
                    lr_str = f"| LR: {curr_lr:.2e}"

                print(f"Ep {ep:<4} | R: {ep_r:>7.1f} | D: {avg_delay:>5.3f} | Succ: {ep_succ:>2} | FPS: {fps} {lr_str}")

            # [ä¿®æ­£] å®šæœŸä¿å­˜å®Œæ•´ Checkpoint
            if ep % 20 == 0:
                if hasattr(agent, 'save_ckpt'):
                    agent.save_ckpt(model_dir, ep)
                elif hasattr(agent, 'save'):
                    agent.save(model_dir)

        except KeyboardInterrupt:
            print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰çŠ¶æ€...")
            if hasattr(agent, 'save_ckpt'): agent.save_ckpt(model_dir, ep)
            return
        except Exception as e:
            print(f"âŒ è®­ç»ƒå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            break

    print(f"âœ… å®éªŒç»“æŸ: {algo_name}")


if __name__ == "__main__":
    # --- å®éªŒå…¥å£ ---
    # 1. å…ˆè·‘ä¸»è§’ (éªŒè¯æ˜¯å¦å¼€å¯äº† Frame Stack)
    # run_experiment("ST-C-MASAC")

    # 2. å†è·‘é…è§’ (éªŒè¯æ˜¯å¦ç¦ç”¨äº† Frame Stack)
    # run_experiment("DDPG")
    run_experiment("Random")